# Imports
import numpy as np 
from numba import jit 

# Setting random seed
np.random.seed(1000)


# Layers
class Dense:
    def __init__(self, inputs_shape, num_neurons):
        self.weights = 0.01 * np.random.randn(inputs_shape, num_neurons)
        self.biases = np.zeros((1, num_neurons))

    def forward(self, inputs):
        output = np.dot(inputs, self.weights) + self.biases
        return output

    def backward(self, inputs, prev_outputs):
        self.dweights = np.dot(inputs.T, prev_outputs) 
        self.dbiases = np.sum(prev_outputs, axis=0, keepdims=True)

        self.dinputs = np.dot(prev_outputs, self.weights.T)


class Flatten:
    def forward(self, x):
        inp_shape = x.shape()

# Activation functions
class ReLU:
    def forward(self, x):
        return np.maximum(x, 0)


    def backward(self, x):
        return x > 0


class Sine:
    def forward(self, x):
        return np.sin(x)

    def backward(self, x):
        return np.cos(x)


class Sigmoid:
    def forward(self, x):
        return (1 / 1 + np.exp(-x))

    def backward(self, x):
        return (1 / 1 + np.exp(-x)) * (1 - (1 / 1 + np.exp(-x)))


class Softmax:
    def forward(self, x):
        return np.exp(x) / np.sum(np.exp(x))


    # Fast execution using jit
    @jit(fastmath=True)
    def backward(self, s):
        a = np.eye(s.shape[-1])
        temp1 = np.zeros((s.shape[0], s.shape[1], s.shape[1]),dtype=np.float32)
        temp2 = np.zeros((s.shape[0], s.shape[1], s.shape[1]),dtype=np.float32)
        for i in range(s.shape[0]):
            for j in range(s.shape[1]):
                for k in range(s.shape[1]):
                    temp1[i,j,k] = s[i,j]*a[j,k]
                    temp2[i,j,k] = s[i,j]*s[i,k]
     
        return temp1-temp2
